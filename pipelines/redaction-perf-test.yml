parameters:
- name: env
  type: string
  default: test
  values: [dev, test, prod]

- name: perfConcurrency
  type: number
  default: 5
- name: perfTotal
  type: number
  default: 5
- name: perfTimeoutSeconds
  type: number
  default: 7200
- name: perfPollSeconds
  type: number
  default: 2

pr: none
trigger: none

resources:
  repositories:
    - repository: templates
      type: github
      endpoint: Planning-Inspectorate
      name: Planning-Inspectorate/common-pipeline-templates
      ref: refs/tags/release/3.24.2

extends:
  template: stages/wrapper_ci.yml@templates
  parameters:
    validateName: Redaction Perf
    globalVariables:
      - template: pipelines/azure-pipelines-variables.yml@self

    validationJobs:
      - name: RunPerfTests
        steps:
          - checkout: self
            clean: true
          - checkout: templates

          # Azure auth: this pattern works (no string embedding)
          - template: ../steps/azure_auth.yml@templates
            parameters:
              subscriptionId: $(SUBSCRIPTION_ID_${{ upper(parameters.env) }})

          - script: |
              set -e
              cd redaction-system
              python3 -m pip install -r redactor/requirements.txt
            displayName: Install requirements

          - script: |
              set -e
              cd redaction-system/redactor
              export PYTHONPATH="$(pwd)"

              # Make Python output unbuffered in ADO logs
              export PYTHONUNBUFFERED=1

              # Pull env-specific pipeline variables (macro expansion), then export as env vars for pytest
              export E2E_FUNCTION_BASE_URL="$(E2E_FUNCTION_BASE_URL_${{ upper(parameters.env) }})"
              export E2E_FUNCTION_KEY="$(FUNCTION_APP_KEY_${{ upper(parameters.env) }})"
              export E2E_CONTAINER_NAME="$(E2E_CONTAINER_NAME_${{ upper(parameters.env) }})"
              export E2E_STORAGE_ACCOUNT="pinsstredaction${{ parameters.env }}uks"
              export E2E_RUN_ID="ado-perf-$(Build.BuildId)"

              # Debug/validation knobs
              export PERF_EXISTS_SAMPLE_EVERY=1
              export PERF_DEBUG_STATUS_JSON_ALWAYS=1

              # (Optional) give the output blob a longer window to appear
              # export PERF_EXISTS_WAIT_S=60
              # export PERF_EXISTS_WAIT_POLL_S=2

              # Perf knobs
              export PERF_CONCURRENCY="${{ parameters.perfConcurrency }}"
              export PERF_TOTAL="${{ parameters.perfTotal }}"
              export PERF_TIMEOUT_S="${{ parameters.perfTimeoutSeconds }}"
              export PERF_POLL_S="${{ parameters.perfPollSeconds }}"

              echo "env=${{ parameters.env }}"
              echo "E2E_FUNCTION_BASE_URL=${E2E_FUNCTION_BASE_URL}"
              echo "E2E_STORAGE_ACCOUNT=${E2E_STORAGE_ACCOUNT}"
              echo "E2E_CONTAINER_NAME=${E2E_CONTAINER_NAME}"
              echo "PERF_CONCURRENCY=${PERF_CONCURRENCY} PERF_TOTAL=${PERF_TOTAL}"
              echo "Has function key? $( [ -n "${E2E_FUNCTION_KEY}" ] && echo yes || echo no )"
              echo "Key length: ${#E2E_FUNCTION_KEY}"


              # Run tests (capture exit code so we can list blobs even on failure)
              set +e
              python3 -m pytest -q -s \
                test/perf_test/test_perf_concurrent_redactions.py::test_concurrent_redactions_perf \
                --junitxml=junit/perf_test_results.xml
              PYTEST_EXIT=$?
              set -e

              echo ""
              echo "Listing outputs under: perf/${E2E_RUN_ID}/out/"
              az storage blob list \
                --account-name "${E2E_STORAGE_ACCOUNT}" \
                --container-name "${E2E_CONTAINER_NAME}" \
                --prefix "perf/${E2E_RUN_ID}/out/" \
                --auth-mode login \
                -o tsv \
                --query "[].name" \
                | sort || true

              exit $PYTEST_EXIT
            displayName: Run perf tests (deployed env)

          - task: PublishTestResults@2
            condition: succeededOrFailed()
            inputs:
              testResultsFiles: '**/perf_test_results.xml'
              mergeTestResults: true
              failTaskOnFailedTests: true
